ü§ñ Laurimate ‚Äì Pepper Robot Campus Assistant
Final Interaction & UI Concept


1. Core Design Principle

Laurimate is designed as a voice-first social robot.

Pepper primarily listens and speaks

This preserves:

natural human‚Äìrobot communication

Pepper‚Äôs social character

accessibility in noisy or quiet environments




2. Welcome Screen & Idle State

Visual Design

Full-screen minimal interface

Soft animated background
Laurimate logo

Behavior

Pepper faces the user

Subtle idle animation

‚ÄúHello! I‚Äôm Laurimate, your campus assistant.‚Äù




User can:


start speaking by tapping on start and 

Central voice bubble (slow breathing animation) appears

laurimate will write listening




3. Language Selection (Global Control)

Placement

Top-right corner

Flag icons:

üá¨üáß English

üá´üáÆ Finnish

üá∏üá™ Swedish

Effect

Changing language updates:

speech recognition language

voice output language

on-screen text captions

Language can be changed at any time.




4. Primary Interaction Mode: Voice (Default)

UI Layout

Large animated voice bubble in the center

Small microphone icon inside the bubble

Live transcribed text shown below the bubble

Bubble States & Animations
State	Description
Idle	Slow pulse (breathing)
Listening	Expanding sound waves
Thinking	Rotating dots
Speaking	Pulse synchronized with voice
Voice Input Flow

User speaks naturally

Bubble reacts to sound

Recognized speech appears as live text:

‚Äúwifi password‚Äù

Listening stops automatically

Processing Logic

Campus JSON knowledge searched first

If information exists ‚Üí direct answer

If not ‚Üí AI fallback (Gemini)

This ensures:

accuracy

speed

responsible AI usage

Voice Output Flow

Pepper speaks the response using robot TTS

Bubble animates while speaking

Caption text appears below for clarity

Example:

‚ÄúUse the Eduroam network. Log in with your Laurea email.‚Äù







5. Idle & Recovery Behavior

After inactivity:

bubble returns to the start animation

Pepper softly says:

‚ÄúI‚Äôm here if you need help.‚Äù

If speech not recognized:

‚ÄúSorry, could you repeat that?‚Äù




6. Concept Summary (One-Sentence Pitch)


Laurimate is a multilingual, voice-first campus assistant for the Pepper robot that provides accurate university information through natural speech interaction, with an optional chat-style typing interface as a fallback.



Overall Flow with Tablet :

[User speaks] 
   ‚Üì
ALSpeechRecognition ‚Üí Python script 
   ‚Üì
Check JSON 
   ‚Üì
Python ‚Üí ALTextToSpeech ‚Üí Pepper speaks
Python ‚Üí ALTabletService ‚Üí Updates HTML bubble + transcription



























-------------------------------------------------------------------------------




1. Use Pepper‚Äôs Existing Capabilities

Pepper already has:

NAOqi framework (Python/Choregraphe)

Text-to-speech (TTS) and speech recognition

Tablet display for simple HTML/JS interfaces

Key idea: Leverage Pepper‚Äôs built-in modules first, only add AI fallback when needed.

2. Core Architecture (Minimal Coding)

Components:

Voice Input ‚Üí Pepper speech recognition

Use ALSpeechRecognition module in Python.

Trigger ‚Äúlistening‚Äù animation while speech is captured.

Process query ‚Üí Local JSON Knowledge Base

Simple JSON file with FAQs (e.g., WiFi, library hours).

Minimal Python code: check if the question exists; return answer.

Fallback AI ‚Üí Gemini / OpenAI API

Only query if JSON has no answer.

Return text answer ‚Üí send to Pepper TTS for speaking.

UI on tablet ‚Üí simple HTML page

Voice bubble animations can be CSS-based.

Transcription ‚Üí update via WebSocket or ALTabletService.

Keyboard overlay ‚Üí basic HTML chat input.

3. Suggested Minimal Stack
Component	Suggested Approach
Voice recognition	Pepper ALSpeechRecognition
Voice output	Pepper ALTextToSpeech
Local knowledge	JSON file + Python search
AI fallback	OpenAI API (text in ‚Üí text out)
Tablet interface	Simple HTML/CSS/JS; WebSocket updates
Animations	CSS animations for breathing, thinking, speaking
Language switch	Reload Pepper speech recognizer with different language code (en/fi/sv)

This avoids writing a full app from scratch or dealing with complex NLP; most logic is Python + HTML/CSS for the tablet.

4. Minimal Python Flow Example
from naoqi import ALProxy
import json

# Connect to Pepper
tts = ALProxy("ALTextToSpeech", "PEPPER_IP", 9559)
sr = ALProxy("ALSpeechRecognition", "PEPPER_IP", 9559)

# Load knowledge base
with open("campus_faq.json") as f:
    knowledge = json.load(f)

def respond_to_query(query):
    query_lower = query.lower()
    if query_lower in knowledge:
        answer = knowledge[query_lower]
    else:
        # Call AI fallback
        answer = call_gpt(query)  # minimal wrapper for AI API
    tts.say(answer)

# Example: start listening
sr.subscribe("Laurimate")


All animations and UI are handled separately on the tablet via HTML/CSS, updating the text bubble via WebSocket when respond_to_query() runs.

5. Tablet UI Tips

Keep it one page HTML:

Voice bubble in center (CSS breathing animation)

Live transcription below

Optional keyboard (hidden by default)

Animations like thinking dots and speaking pulses are simple CSS keyframes.

Language selection ‚Üí just update the query language code for Pepper‚Äôs ALSpeechRecognition.

6. Extra Simplifications

Skip typing mode initially; can just use browser alert for fallback.

Preload JSON knowledge for common campus info ‚Üí AI only for rare queries.

Keep voice-first: user taps once ‚Üí Pepper listens ‚Üí processes ‚Üí speaks.

‚úÖ Summary:

Python for Pepper logic (voice input ‚Üí JSON lookup ‚Üí AI fallback ‚Üí TTS output)

HTML/CSS on tablet for minimal, animated UI (voice bubble, captions, keyboard overlay)

No heavy frameworks needed, just Python + WebSocket + simple HTML/CSS/JS

Scalable: JSON can grow, AI fallback handles anything else





https://cloud.aldebaran-robotics.com/manage/robots/36257/ : login page for robot softbank robotics

https://github.com/sunzhida/COMP4461_2017Fall_Lab4?tab=readme-ov-file : GitHub page for making the robot speak